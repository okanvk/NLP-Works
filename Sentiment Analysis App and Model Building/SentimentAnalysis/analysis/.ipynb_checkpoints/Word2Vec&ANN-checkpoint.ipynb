{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.utils import tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data_clean.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alway wrote seri complet stinkfest jim belushi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1st watch 10dirstev purcel typic mari kate ash...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>movi poorli written direct fell asleep minut m...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>interest thing miryang secret sunshin actor je...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>first read berlin meer didnt expect much thoug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  label\n",
       "0  alway wrote seri complet stinkfest jim belushi...      0\n",
       "1  1st watch 10dirstev purcel typic mari kate ash...      0\n",
       "2  movi poorli written direct fell asleep minut m...      0\n",
       "3  interest thing miryang secret sunshin actor je...      1\n",
       "4  first read berlin meer didnt expect much thoug...      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    24884\n",
       "0    24698\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_label(review):\n",
    "    review_text = review\n",
    "    words = review_text.split()\n",
    "    return (words)\n",
    "def labels_to_sentences(label):\n",
    "    raw_sentences = sent_tokenize(label.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(preprocess_label(raw_sentence))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['alway',\n",
       "  'wrote',\n",
       "  'seri',\n",
       "  'complet',\n",
       "  'stinkfest',\n",
       "  'jim',\n",
       "  'belushi',\n",
       "  'involv',\n",
       "  'heavili',\n",
       "  'one',\n",
       "  'day',\n",
       "  'tragic',\n",
       "  'happenst',\n",
       "  'occur',\n",
       "  'white',\n",
       "  'sox',\n",
       "  'game',\n",
       "  'end',\n",
       "  'realiz',\n",
       "  'remot',\n",
       "  'way',\n",
       "  'side',\n",
       "  'room',\n",
       "  'somehow',\n",
       "  'could',\n",
       "  'gotten',\n",
       "  'walk',\n",
       "  'across',\n",
       "  'room',\n",
       "  'get',\n",
       "  'remot',\n",
       "  'even',\n",
       "  'tv',\n",
       "  'turn',\n",
       "  'channel',\n",
       "  'get',\n",
       "  'walk',\n",
       "  'across',\n",
       "  'countri',\n",
       "  'watch',\n",
       "  'tv',\n",
       "  'anoth',\n",
       "  'state',\n",
       "  'nut',\n",
       "  'said',\n",
       "  'decid',\n",
       "  'hang',\n",
       "  'tight',\n",
       "  'couch',\n",
       "  'take',\n",
       "  'whatev',\n",
       "  'fate',\n",
       "  'store',\n",
       "  'fate',\n",
       "  'store',\n",
       "  'episod',\n",
       "  'show',\n",
       "  'episod',\n",
       "  'rememb',\n",
       "  'littl',\n",
       "  'except',\n",
       "  'made',\n",
       "  'broad',\n",
       "  'gener',\n",
       "  'sweep',\n",
       "  'blanket',\n",
       "  'judgment',\n",
       "  'base',\n",
       "  'zero',\n",
       "  'object',\n",
       "  'experienti',\n",
       "  'evid',\n",
       "  'noth',\n",
       "  'whatsoev',\n",
       "  'back',\n",
       "  'opinion',\n",
       "  'complet',\n",
       "  'right',\n",
       "  'show',\n",
       "  'total',\n",
       "  'crudpi',\n",
       "  'belushi',\n",
       "  'comed',\n",
       "  'deliveri',\n",
       "  'hairi',\n",
       "  'lighthous',\n",
       "  'foghorn',\n",
       "  'women',\n",
       "  'physic',\n",
       "  'attract',\n",
       "  'stepfordi',\n",
       "  'elicit',\n",
       "  'real',\n",
       "  'feel',\n",
       "  'viewer',\n",
       "  'absolut',\n",
       "  'reason',\n",
       "  'stop',\n",
       "  'run',\n",
       "  'local',\n",
       "  'tv',\n",
       "  'station',\n",
       "  'gasolin',\n",
       "  'flamethrow',\n",
       "  'send',\n",
       "  'everi',\n",
       "  'copi',\n",
       "  'mutt',\n",
       "  'howl',\n",
       "  'back',\n",
       "  'hell',\n",
       "  'br',\n",
       "  'br',\n",
       "  'except',\n",
       "  'br',\n",
       "  'br',\n",
       "  'except',\n",
       "  'wonder',\n",
       "  'comic',\n",
       "  'sti',\n",
       "  'ling',\n",
       "  'larri',\n",
       "  'joe',\n",
       "  'campbel',\n",
       "  'america',\n",
       "  'greatest',\n",
       "  'comic',\n",
       "  'charact',\n",
       "  'actor',\n",
       "  'guy',\n",
       "  'play',\n",
       "  'belushi',\n",
       "  'brotherinlaw',\n",
       "  'andi',\n",
       "  'gold',\n",
       "  'good',\n",
       "  'realli',\n",
       "  'well',\n",
       "  'asid',\n",
       "  'funni',\n",
       "  'job',\n",
       "  'make',\n",
       "  'belushi',\n",
       "  'look',\n",
       "  'good',\n",
       "  'that',\n",
       "  'like',\n",
       "  'tri',\n",
       "  'make',\n",
       "  'butt',\n",
       "  'wart',\n",
       "  'look',\n",
       "  'good',\n",
       "  'campbel',\n",
       "  'pull',\n",
       "  'style',\n",
       "  'someon',\n",
       "  'invent',\n",
       "  'nobel',\n",
       "  'prize',\n",
       "  'comic',\n",
       "  'buffooneri',\n",
       "  'win',\n",
       "  'everi',\n",
       "  'year',\n",
       "  'without',\n",
       "  'larri',\n",
       "  'joe',\n",
       "  'show',\n",
       "  'would',\n",
       "  'consist',\n",
       "  'slightli',\n",
       "  'vacant',\n",
       "  'look',\n",
       "  'courtney',\n",
       "  'thornesmith',\n",
       "  'smack',\n",
       "  'belushi',\n",
       "  'head',\n",
       "  'fri',\n",
       "  'pan',\n",
       "  'altern',\n",
       "  'beat',\n",
       "  'chest',\n",
       "  'play',\n",
       "  'straw',\n",
       "  'floor',\n",
       "  'cage',\n",
       "  'star',\n",
       "  'larri',\n",
       "  'joe',\n",
       "  'campbel',\n",
       "  'design',\n",
       "  'comed',\n",
       "  'bacon',\n",
       "  'improv',\n",
       "  'flavor',\n",
       "  'everyth',\n",
       "  'he']]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_to_sentences(data['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49582"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences = []\n",
    "for review in data[\"text\"]:\n",
    "    if type(review) == float:\n",
    "        continue\n",
    "    sentences += labels_to_sentences(review)\n",
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sent = [row.split() for row in data['text'] if not type(row) == float]\n",
    "#phrases = Phrases(sent, min_count=1, progress_per=10000)\n",
    "#sentences = phrases[sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_features = 300  \n",
    "min_word_count = 3\n",
    "num_workers = 4     \n",
    "context = 10       \n",
    "downsampling = 1e-3 # (0.001) \n",
    "\n",
    "from gensim.models import word2vec\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling,\n",
    "                          sg = 1\n",
    "                         )\n",
    "\n",
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar(\"awsom\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def featureVecMethod(words, model, num_features):\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    \n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "data_vects = []\n",
    "for review in data['text']:\n",
    "        data_vects.append(preprocess_label(review))\n",
    "    \n",
    "dataAvgVects = getAvgFeatureVecs(data_vects, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVects = []\n",
    "for item in dataAvgVects:\n",
    "    item[np.isnan(item)] = 0\n",
    "    dataVects.append(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataVects = np.asarray(dataVects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelBinarizer()\n",
    "y = encoder.fit_transform(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dataVects,y,test_size=0.2,random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = Sequential()\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dropout(0.2))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "classifier.add(Dense(units=128,activation='relu'))\n",
    "\n",
    "classifier.add(Dense(units=1,activation='sigmoid'))\n",
    "\n",
    "classifier.compile(optimizer='adam',\n",
    "              loss='binary_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 39665 samples, validate on 9917 samples\n",
      "Epoch 1/30\n",
      "39665/39665 [==============================] - 14s 346us/sample - loss: 0.3662 - accuracy: 0.8366 - val_loss: 0.3424 - val_accuracy: 0.8488\n",
      "Epoch 2/30\n",
      "39665/39665 [==============================] - 11s 285us/sample - loss: 0.3050 - accuracy: 0.8751 - val_loss: 0.2979 - val_accuracy: 0.8637\n",
      "Epoch 3/30\n",
      "39665/39665 [==============================] - 12s 309us/sample - loss: 0.2958 - accuracy: 0.8798 - val_loss: 0.3387 - val_accuracy: 0.8637\n",
      "Epoch 4/30\n",
      "39665/39665 [==============================] - 12s 298us/sample - loss: 0.2967 - accuracy: 0.8795 - val_loss: 0.2885 - val_accuracy: 0.8850\n",
      "Epoch 5/30\n",
      "39665/39665 [==============================] - 12s 304us/sample - loss: 0.2868 - accuracy: 0.8829 - val_loss: 0.2735 - val_accuracy: 0.8876\n",
      "Epoch 6/30\n",
      "39665/39665 [==============================] - 11s 267us/sample - loss: 0.2874 - accuracy: 0.8816 - val_loss: 0.2866 - val_accuracy: 0.8739\n",
      "Epoch 7/30\n",
      "39665/39665 [==============================] - 11s 272us/sample - loss: 0.2822 - accuracy: 0.8849 - val_loss: 0.2718 - val_accuracy: 0.8878\n",
      "Epoch 8/30\n",
      "39665/39665 [==============================] - 11s 280us/sample - loss: 0.2815 - accuracy: 0.8841 - val_loss: 0.2797 - val_accuracy: 0.8862\n",
      "Epoch 9/30\n",
      "39665/39665 [==============================] - 11s 271us/sample - loss: 0.2776 - accuracy: 0.8865 - val_loss: 0.2710 - val_accuracy: 0.8880\n",
      "Epoch 10/30\n",
      "39665/39665 [==============================] - 12s 306us/sample - loss: 0.2794 - accuracy: 0.8855 - val_loss: 0.2789 - val_accuracy: 0.8887\n",
      "Epoch 11/30\n",
      "39665/39665 [==============================] - 13s 318us/sample - loss: 0.2744 - accuracy: 0.8880 - val_loss: 0.2693 - val_accuracy: 0.8899\n",
      "Epoch 12/30\n",
      "39665/39665 [==============================] - 12s 311us/sample - loss: 0.2714 - accuracy: 0.8883 - val_loss: 0.2806 - val_accuracy: 0.8838\n",
      "Epoch 13/30\n",
      "39665/39665 [==============================] - 12s 295us/sample - loss: 0.2708 - accuracy: 0.8894 - val_loss: 0.2746 - val_accuracy: 0.8883\n",
      "Epoch 14/30\n",
      "39665/39665 [==============================] - 11s 272us/sample - loss: 0.2678 - accuracy: 0.8900 - val_loss: 0.3284 - val_accuracy: 0.8536\n",
      "Epoch 15/30\n",
      "39665/39665 [==============================] - 11s 270us/sample - loss: 0.2678 - accuracy: 0.8892 - val_loss: 0.3105 - val_accuracy: 0.8722\n",
      "Epoch 16/30\n",
      "39665/39665 [==============================] - 11s 282us/sample - loss: 0.2667 - accuracy: 0.8908 - val_loss: 0.2640 - val_accuracy: 0.8898\n",
      "Epoch 17/30\n",
      "39665/39665 [==============================] - 12s 311us/sample - loss: 0.2634 - accuracy: 0.8922 - val_loss: 0.2691 - val_accuracy: 0.8906\n",
      "Epoch 18/30\n",
      "39665/39665 [==============================] - 12s 294us/sample - loss: 0.2660 - accuracy: 0.8923 - val_loss: 0.2792 - val_accuracy: 0.8783\n",
      "Epoch 19/30\n",
      "39665/39665 [==============================] - 12s 293us/sample - loss: 0.2599 - accuracy: 0.8936 - val_loss: 0.2916 - val_accuracy: 0.8831\n",
      "Epoch 20/30\n",
      "39665/39665 [==============================] - 12s 308us/sample - loss: 0.2584 - accuracy: 0.8931 - val_loss: 0.2732 - val_accuracy: 0.8916\n",
      "Epoch 21/30\n",
      "39665/39665 [==============================] - 13s 326us/sample - loss: 0.2586 - accuracy: 0.8953 - val_loss: 0.2892 - val_accuracy: 0.8703\n",
      "Epoch 22/30\n",
      "39665/39665 [==============================] - 12s 304us/sample - loss: 0.2584 - accuracy: 0.8936 - val_loss: 0.2817 - val_accuracy: 0.8794\n",
      "Epoch 23/30\n",
      "39665/39665 [==============================] - 13s 327us/sample - loss: 0.2562 - accuracy: 0.8967 - val_loss: 0.2678 - val_accuracy: 0.8856\n",
      "Epoch 24/30\n",
      "39665/39665 [==============================] - 12s 310us/sample - loss: 0.2531 - accuracy: 0.8961 - val_loss: 0.2723 - val_accuracy: 0.8920\n",
      "Epoch 25/30\n",
      "39665/39665 [==============================] - 12s 300us/sample - loss: 0.2528 - accuracy: 0.8958 - val_loss: 0.2716 - val_accuracy: 0.8873\n",
      "Epoch 26/30\n",
      "39665/39665 [==============================] - 13s 324us/sample - loss: 0.2522 - accuracy: 0.8974 - val_loss: 0.2744 - val_accuracy: 0.8823\n",
      "Epoch 27/30\n",
      "39665/39665 [==============================] - 12s 296us/sample - loss: 0.2507 - accuracy: 0.8955 - val_loss: 0.2927 - val_accuracy: 0.8719\n",
      "Epoch 28/30\n",
      "39665/39665 [==============================] - 12s 298us/sample - loss: 0.2501 - accuracy: 0.8972 - val_loss: 0.2760 - val_accuracy: 0.8836\n",
      "Epoch 29/30\n",
      "39665/39665 [==============================] - 13s 321us/sample - loss: 0.2474 - accuracy: 0.8976 - val_loss: 0.2659 - val_accuracy: 0.8914\n",
      "Epoch 30/30\n",
      "39665/39665 [==============================] - 12s 308us/sample - loss: 0.2520 - accuracy: 0.8967 - val_loss: 0.2654 - val_accuracy: 0.8900\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1a93717050>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.fit(x=X_train, \n",
    "          y=y_train, \n",
    "          epochs=30,\n",
    "          validation_data=(X_test, y_test), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"word2vec.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.save(\"sentiment_classifier.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
