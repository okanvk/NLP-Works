{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextGenerationAccordingToSonnets.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "TP8eU-Agsl2o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding,GRU,Dense"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6BlF8GKCwZXH",
        "colab_type": "text"
      },
      "source": [
        "Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2zNrAJ4uhQ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_to_file = \"shakespeare.txt\"\n",
        "# We're gonna use shakespeare sonnet for data\n",
        "text = open(path_to_file,\"r\").read()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P3iEvZVCu8Zo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "8bca6bcd-160c-46e6-90ba-f7d934846eb6"
      },
      "source": [
        "print(text[:700]) # as we can see above every 650-700 character we have a sonet"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bud buriest thy content,\n",
            "  And tender churl mak'st waste in niggarding:\n",
            "    Pity the world, or else this glutton be,\n",
            "    To eat the world's due, by the grave and thee.\n",
            "\n",
            "\n",
            "                     2\n",
            "  When fo\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gj5Xm7-Fu-He",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab = sorted(set(text)) # we're gonna build our text generation with character based"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF9_v-IfvOC7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "49e7cece-0da7-4c7e-d9a5-008204c37e8b"
      },
      "source": [
        "vocab #Â therefore our vocabulary consist characters not words"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n',\n",
              " ' ',\n",
              " '!',\n",
              " '\"',\n",
              " '&',\n",
              " \"'\",\n",
              " '(',\n",
              " ')',\n",
              " ',',\n",
              " '-',\n",
              " '.',\n",
              " '0',\n",
              " '1',\n",
              " '2',\n",
              " '3',\n",
              " '4',\n",
              " '5',\n",
              " '6',\n",
              " '7',\n",
              " '8',\n",
              " '9',\n",
              " ':',\n",
              " ';',\n",
              " '<',\n",
              " '>',\n",
              " '?',\n",
              " 'A',\n",
              " 'B',\n",
              " 'C',\n",
              " 'D',\n",
              " 'E',\n",
              " 'F',\n",
              " 'G',\n",
              " 'H',\n",
              " 'I',\n",
              " 'J',\n",
              " 'K',\n",
              " 'L',\n",
              " 'M',\n",
              " 'N',\n",
              " 'O',\n",
              " 'P',\n",
              " 'Q',\n",
              " 'R',\n",
              " 'S',\n",
              " 'T',\n",
              " 'U',\n",
              " 'V',\n",
              " 'W',\n",
              " 'X',\n",
              " 'Y',\n",
              " 'Z',\n",
              " '[',\n",
              " ']',\n",
              " '_',\n",
              " '`',\n",
              " 'a',\n",
              " 'b',\n",
              " 'c',\n",
              " 'd',\n",
              " 'e',\n",
              " 'f',\n",
              " 'g',\n",
              " 'h',\n",
              " 'i',\n",
              " 'j',\n",
              " 'k',\n",
              " 'l',\n",
              " 'm',\n",
              " 'n',\n",
              " 'o',\n",
              " 'p',\n",
              " 'q',\n",
              " 'r',\n",
              " 's',\n",
              " 't',\n",
              " 'u',\n",
              " 'v',\n",
              " 'w',\n",
              " 'x',\n",
              " 'y',\n",
              " 'z',\n",
              " '|',\n",
              " '}']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kz_CNzWYvOwu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "562228a3-79f8-4112-e0d4-8060cb2113aa"
      },
      "source": [
        "len(vocab) # Lenght of vocabulary"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "84"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WFPdvQYQw9Ir",
        "colab_type": "text"
      },
      "source": [
        "Processing\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mp7gNEnFvTkc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b490a1d5-8645-42e0-91e2-e5bae3d5ba1c"
      },
      "source": [
        "# we create dictionary for value for every character\n",
        "\n",
        "for pair in enumerate(vocab):\n",
        "  print(pair)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(0, '\\n')\n",
            "(1, ' ')\n",
            "(2, '!')\n",
            "(3, '\"')\n",
            "(4, '&')\n",
            "(5, \"'\")\n",
            "(6, '(')\n",
            "(7, ')')\n",
            "(8, ',')\n",
            "(9, '-')\n",
            "(10, '.')\n",
            "(11, '0')\n",
            "(12, '1')\n",
            "(13, '2')\n",
            "(14, '3')\n",
            "(15, '4')\n",
            "(16, '5')\n",
            "(17, '6')\n",
            "(18, '7')\n",
            "(19, '8')\n",
            "(20, '9')\n",
            "(21, ':')\n",
            "(22, ';')\n",
            "(23, '<')\n",
            "(24, '>')\n",
            "(25, '?')\n",
            "(26, 'A')\n",
            "(27, 'B')\n",
            "(28, 'C')\n",
            "(29, 'D')\n",
            "(30, 'E')\n",
            "(31, 'F')\n",
            "(32, 'G')\n",
            "(33, 'H')\n",
            "(34, 'I')\n",
            "(35, 'J')\n",
            "(36, 'K')\n",
            "(37, 'L')\n",
            "(38, 'M')\n",
            "(39, 'N')\n",
            "(40, 'O')\n",
            "(41, 'P')\n",
            "(42, 'Q')\n",
            "(43, 'R')\n",
            "(44, 'S')\n",
            "(45, 'T')\n",
            "(46, 'U')\n",
            "(47, 'V')\n",
            "(48, 'W')\n",
            "(49, 'X')\n",
            "(50, 'Y')\n",
            "(51, 'Z')\n",
            "(52, '[')\n",
            "(53, ']')\n",
            "(54, '_')\n",
            "(55, '`')\n",
            "(56, 'a')\n",
            "(57, 'b')\n",
            "(58, 'c')\n",
            "(59, 'd')\n",
            "(60, 'e')\n",
            "(61, 'f')\n",
            "(62, 'g')\n",
            "(63, 'h')\n",
            "(64, 'i')\n",
            "(65, 'j')\n",
            "(66, 'k')\n",
            "(67, 'l')\n",
            "(68, 'm')\n",
            "(69, 'n')\n",
            "(70, 'o')\n",
            "(71, 'p')\n",
            "(72, 'q')\n",
            "(73, 'r')\n",
            "(74, 's')\n",
            "(75, 't')\n",
            "(76, 'u')\n",
            "(77, 'v')\n",
            "(78, 'w')\n",
            "(79, 'x')\n",
            "(80, 'y')\n",
            "(81, 'z')\n",
            "(82, '|')\n",
            "(83, '}')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMQq5CvjxXme",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dictionary which give us index according to char\n",
        "char_to_ind = {char:ind for ind,char in enumerate(vocab)}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mT7DElWxXjy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# numpy array which give us char according to index\n",
        "ind_to_char = np.array(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRWTjeCDxXhD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7a4d95be-fc1f-4d7d-af00-7851db1448c6"
      },
      "source": [
        "char_to_ind[\"a\"]"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "56"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwHi2j8qxXei",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce9c5e84-0caf-44d3-dbd2-2ad5d1eaecb0"
      },
      "source": [
        "ind_to_char[56]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'a'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouZkIdPmxXb5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we transform our full text to indexes\n",
        "encoded_text = np.array([char_to_ind[c] for c in text])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ZrXqBmjxXZS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cfd39ab6-35b7-466e-f32b-fef6f3496ff8"
      },
      "source": [
        "encoded_text.shape\n",
        "# we have 5445609 characters. "
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(5445609,)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gIun-m5ExXWx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample = text[:500]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECCl5lj4xXUI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "cff92866-e1f8-4b29-b607-3970ef5a6e7e"
      },
      "source": [
        "sample"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\n                     1\\n  From fairest creatures we desire increase,\\n  That thereby beauty's rose might never die,\\n  But as the riper should by time decease,\\n  His tender heir might bear his memory:\\n  But thou contracted to thine own bright eyes,\\n  Feed'st thy light's flame with self-substantial fuel,\\n  Making a famine where abundance lies,\\n  Thy self thy foe, to thy sweet self too cruel:\\n  Thou that art now the world's fresh ornament,\\n  And only herald to the gaudy spring,\\n  Within thine own bu\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rXDSKQfWxXRh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 527
        },
        "outputId": "32053b99-f2af-47f1-9d6f-ae333fae31e6"
      },
      "source": [
        "encoded_text[:500]"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
              "        1,  1,  1,  1,  1, 12,  0,  1,  1, 31, 73, 70, 68,  1, 61, 56, 64,\n",
              "       73, 60, 74, 75,  1, 58, 73, 60, 56, 75, 76, 73, 60, 74,  1, 78, 60,\n",
              "        1, 59, 60, 74, 64, 73, 60,  1, 64, 69, 58, 73, 60, 56, 74, 60,  8,\n",
              "        0,  1,  1, 45, 63, 56, 75,  1, 75, 63, 60, 73, 60, 57, 80,  1, 57,\n",
              "       60, 56, 76, 75, 80,  5, 74,  1, 73, 70, 74, 60,  1, 68, 64, 62, 63,\n",
              "       75,  1, 69, 60, 77, 60, 73,  1, 59, 64, 60,  8,  0,  1,  1, 27, 76,\n",
              "       75,  1, 56, 74,  1, 75, 63, 60,  1, 73, 64, 71, 60, 73,  1, 74, 63,\n",
              "       70, 76, 67, 59,  1, 57, 80,  1, 75, 64, 68, 60,  1, 59, 60, 58, 60,\n",
              "       56, 74, 60,  8,  0,  1,  1, 33, 64, 74,  1, 75, 60, 69, 59, 60, 73,\n",
              "        1, 63, 60, 64, 73,  1, 68, 64, 62, 63, 75,  1, 57, 60, 56, 73,  1,\n",
              "       63, 64, 74,  1, 68, 60, 68, 70, 73, 80, 21,  0,  1,  1, 27, 76, 75,\n",
              "        1, 75, 63, 70, 76,  1, 58, 70, 69, 75, 73, 56, 58, 75, 60, 59,  1,\n",
              "       75, 70,  1, 75, 63, 64, 69, 60,  1, 70, 78, 69,  1, 57, 73, 64, 62,\n",
              "       63, 75,  1, 60, 80, 60, 74,  8,  0,  1,  1, 31, 60, 60, 59,  5, 74,\n",
              "       75,  1, 75, 63, 80,  1, 67, 64, 62, 63, 75,  5, 74,  1, 61, 67, 56,\n",
              "       68, 60,  1, 78, 64, 75, 63,  1, 74, 60, 67, 61,  9, 74, 76, 57, 74,\n",
              "       75, 56, 69, 75, 64, 56, 67,  1, 61, 76, 60, 67,  8,  0,  1,  1, 38,\n",
              "       56, 66, 64, 69, 62,  1, 56,  1, 61, 56, 68, 64, 69, 60,  1, 78, 63,\n",
              "       60, 73, 60,  1, 56, 57, 76, 69, 59, 56, 69, 58, 60,  1, 67, 64, 60,\n",
              "       74,  8,  0,  1,  1, 45, 63, 80,  1, 74, 60, 67, 61,  1, 75, 63, 80,\n",
              "        1, 61, 70, 60,  8,  1, 75, 70,  1, 75, 63, 80,  1, 74, 78, 60, 60,\n",
              "       75,  1, 74, 60, 67, 61,  1, 75, 70, 70,  1, 58, 73, 76, 60, 67, 21,\n",
              "        0,  1,  1, 45, 63, 70, 76,  1, 75, 63, 56, 75,  1, 56, 73, 75,  1,\n",
              "       69, 70, 78,  1, 75, 63, 60,  1, 78, 70, 73, 67, 59,  5, 74,  1, 61,\n",
              "       73, 60, 74, 63,  1, 70, 73, 69, 56, 68, 60, 69, 75,  8,  0,  1,  1,\n",
              "       26, 69, 59,  1, 70, 69, 67, 80,  1, 63, 60, 73, 56, 67, 59,  1, 75,\n",
              "       70,  1, 75, 63, 60,  1, 62, 56, 76, 59, 80,  1, 74, 71, 73, 64, 69,\n",
              "       62,  8,  0,  1,  1, 48, 64, 75, 63, 64, 69,  1, 75, 63, 64, 69, 60,\n",
              "        1, 70, 78, 69,  1, 57, 76])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ov5MLjSrxXO5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "seq_len = 120\n",
        "# our sequence length for every sequence"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovvoAn_txXMT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "total_number_seq = len(text) // (seq_len + 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzwENElVxXKW",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7d6f6f21-9084-40bc-fb0e-8afa45ab3200"
      },
      "source": [
        "total_number_seq\n",
        "# we're gonna have 45005 sequences"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "45005"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7iKKhX7xXG4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "# we create char dataset from tensorflow Dataset using encoded text"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Ew_YidnxXEX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences = char_dataset.batch(seq_len+1,drop_remainder=True)\n",
        "# we're creating our sequencess with batch according to sequence length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JIzJMV3xW4l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_seq_targets(seq):\n",
        "  input_txt = seq[:-1] # Okan i\n",
        "  target_txt = seq[1:] # kan is\n",
        "  return input_txt,target_txt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw55vE121ChV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mapping sequences to create input and target data\n",
        "dataset = sequences.map(create_seq_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P-4Z29yG1Ceg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "867226c4-06e7-4e03-cd44-860f75c71e6c"
      },
      "source": [
        "for input_txt,target_txt in dataset.take(1):\n",
        "  print(input_txt.numpy())\n",
        "  print(\"\".join(ind_to_char[input_txt.numpy()]))\n",
        "  print(\"\\n\")\n",
        "  print(target_txt.numpy())\n",
        "  print(\"\".join(ind_to_char[target_txt.numpy()]))\n",
        "  # simple input and target data output"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
            "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But\n",
            "\n",
            "\n",
            "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
            "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
            " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
            " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
            "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XVnCWcYv1Cbp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128\n",
        "buffer_size = 10000\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size,drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7K38SFh1CZf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "c8d1bbd6-4e11-4a93-acd4-65413430cd4b"
      },
      "source": [
        "dataset\n",
        "# we have batches one batch containes 128 sequence\n",
        "# we have lenght of sequence : 120"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((128, 120), (128, 120)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZsuvXji1CWe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for embedding layer\n",
        "embed_dim = 64\n",
        "rnn_neurons = 1536"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qTQZDQT31CT1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sparse_cat_loss(y_true,y_pred):\n",
        "  return sparse_categorical_crossentropy(y_true,y_pred,from_logits=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zlvJ1KBl1CRS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_model(vocab_size,embed_dim,rnn_neurons,batch_size):\n",
        "\n",
        "  model = Sequential()\n",
        "\n",
        "  model.add(Embedding(vocab_size,embed_dim,batch_input_shape=[batch_size,None]))\n",
        "  # vocab_size -> input dimension\n",
        "  # embed_dim -> output dimension\n",
        "  # batch_input_size -> specify batch_size\n",
        "  model.add(GRU(rnn_neurons,return_sequences=True,stateful=True,recurrent_initializer=\"glorot_uniform\"))\n",
        "  # return_sequences = true -> we return last output sequence\n",
        "  #Â stateful = true -> we keep current state\n",
        "  model.add(Dense(vocab_size))\n",
        "  # output layer\n",
        "\n",
        "  model.compile(\"adam\",loss=sparse_cat_loss)\n",
        "\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDpH_TRx1CO2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "vocab_size = len(vocab)\n",
        "model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size)\n",
        "# we build our model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcioSA8f1CMF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "80072ccf-ed2e-474c-9420-215ae62ec7c8"
      },
      "source": [
        "model.summary()\n",
        "# model summary"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (128, None, 64)           5376      \n",
            "_________________________________________________________________\n",
            "gru (GRU)                    (128, None, 1536)         7382016   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (128, None, 84)           129108    \n",
            "=================================================================\n",
            "Total params: 7,516,500\n",
            "Trainable params: 7,516,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVHE9cBy1CJi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epochs = 30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zY9HZ8U01CG4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4c3d0e43-687e-4c8e-ef53-37ae0905c92e"
      },
      "source": [
        "# we predict random batch before training\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "\n",
        "  print(example_batch_predictions.shape, \" <=== (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(128, 120, 84)  <=== (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6HvWRSEl1CEY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "42547f47-0a1d-47bd-f6b7-49fe1d4ab1c3"
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "print(\"Given the input seq: \\n\")\n",
        "print(\"\".join(ind_to_char[input_example_batch[0]]))\n",
        "print('\\n')\n",
        "print(\"Next Char Predictions: \\n\")\n",
        "print(\"\".join(ind_to_char[sampled_indices ]))\n"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Given the input seq: \n",
            "\n",
            "\n",
            "    Fare you well, my dove!\n",
            "  Laer. Hadst thou thy wits, and didst persuade revenge,\n",
            "    It could not move thus.  \n",
            "  Op\n",
            "\n",
            "\n",
            "Next Char Predictions: \n",
            "\n",
            "mgZz0OcUX\n",
            "X&;1CITEZ|uJtwiUWci6fGibvTSOF>]7\"!`fSFOGYOlLl\"xBhmf>yV\"TI?7.KQSF)}o|7sxE!pM7I35hI`9\"2}ncAsHX(iY6rxu'ClfQ\"`5;w2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWbAFBk_1B6u",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0a3f0dd1-d3c4-4a34-9b12-7a83bc9b5341"
      },
      "source": [
        "model.fit(dataset,epochs=epochs)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "351/351 [==============================] - 80s 227ms/step - loss: 2.4586\n",
            "Epoch 2/30\n",
            "351/351 [==============================] - 80s 229ms/step - loss: 1.6453\n",
            "Epoch 3/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 1.4037\n",
            "Epoch 4/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 1.2994\n",
            "Epoch 5/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 1.2409\n",
            "Epoch 6/30\n",
            "351/351 [==============================] - 80s 229ms/step - loss: 1.1996\n",
            "Epoch 7/30\n",
            "351/351 [==============================] - 80s 229ms/step - loss: 1.1688\n",
            "Epoch 8/30\n",
            "351/351 [==============================] - 80s 228ms/step - loss: 1.1421\n",
            "Epoch 9/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 1.1189\n",
            "Epoch 10/30\n",
            "351/351 [==============================] - 81s 231ms/step - loss: 1.0976\n",
            "Epoch 11/30\n",
            "351/351 [==============================] - 81s 231ms/step - loss: 1.0771\n",
            "Epoch 12/30\n",
            "351/351 [==============================] - 81s 231ms/step - loss: 1.0572\n",
            "Epoch 13/30\n",
            "351/351 [==============================] - 81s 229ms/step - loss: 1.0382\n",
            "Epoch 14/30\n",
            "351/351 [==============================] - 80s 229ms/step - loss: 1.0199\n",
            "Epoch 15/30\n",
            "351/351 [==============================] - 80s 228ms/step - loss: 1.0025\n",
            "Epoch 16/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 0.9862\n",
            "Epoch 17/30\n",
            "351/351 [==============================] - 81s 231ms/step - loss: 0.9713\n",
            "Epoch 18/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 0.9576\n",
            "Epoch 19/30\n",
            "351/351 [==============================] - 81s 231ms/step - loss: 0.9457\n",
            "Epoch 20/30\n",
            "351/351 [==============================] - 80s 229ms/step - loss: 0.9347\n",
            "Epoch 21/30\n",
            "351/351 [==============================] - 80s 229ms/step - loss: 0.9254\n",
            "Epoch 22/30\n",
            "351/351 [==============================] - 80s 228ms/step - loss: 0.9176\n",
            "Epoch 23/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 0.9117\n",
            "Epoch 24/30\n",
            "351/351 [==============================] - 81s 231ms/step - loss: 0.9061\n",
            "Epoch 25/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 0.9032\n",
            "Epoch 26/30\n",
            "351/351 [==============================] - 81s 231ms/step - loss: 0.8999\n",
            "Epoch 27/30\n",
            "351/351 [==============================] - 80s 229ms/step - loss: 0.8992\n",
            "Epoch 28/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 0.8980\n",
            "Epoch 29/30\n",
            "351/351 [==============================] - 80s 228ms/step - loss: 0.8987\n",
            "Epoch 30/30\n",
            "351/351 [==============================] - 81s 230ms/step - loss: 0.9000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb65e10b080>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNlsdAf0Oga7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Â In first model experience epoch was 40 losss was 1.23.\n",
        "# So i find boundary loss increase which was 30. epoch and train new model with 30 epochs\n",
        "#As we can see above after 28.th epoch model loss is increasing therefore we should stop here."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AqcAhqDOPYjq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.save('my_model.h5') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cjiTl3ChP5Um",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "created_model = create_model(vocab_size,embed_dim,rnn_neurons,batch_size=1)\n",
        " # we specify batch size because we give restricted data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cG_xjlaQRiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "created_model.load_weights(\"my_model.h5\")\n",
        "# we load our weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1q0IZInZQU6R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "created_model.build(tf.TensorShape([1,None]))\n",
        "#Â we build our model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF8cZBEWQf21",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "16e98d26-68ae-4552-c7d8-9d568f9cc968"
      },
      "source": [
        "created_model.summary()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 64)             5376      \n",
            "_________________________________________________________________\n",
            "gru_1 (GRU)                  (1, None, 1536)           7382016   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 84)             129108    \n",
            "=================================================================\n",
            "Total params: 7,516,500\n",
            "Trainable params: 7,516,500\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CnukyqJbQf0p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model,start_seed,gen_size=500,temp=1.0):\n",
        "  \n",
        "  num_generate = gen_size\n",
        "  \n",
        "  input_eval = [char_to_ind[s] for s in start_seed]\n",
        "  # we vectorize our start seed\n",
        "\n",
        "  input_eval = tf.expand_dims(input_eval,0)\n",
        "  # expand dimensions to correct shape\n",
        "\n",
        "  text_generated = []\n",
        "\n",
        "  temperature = temp\n",
        "\n",
        "  model.reset_states()\n",
        "  # reset states\n",
        "\n",
        "  # we iterate until the reach num_generate\n",
        "  for i in range(num_generate):\n",
        "\n",
        "    #we get prediction\n",
        "    predictions = model(input_eval)\n",
        "\n",
        "    # we use squeezing for undo expand dimensions\n",
        "    predictions = tf.squeeze(predictions,0)\n",
        "\n",
        "    # actual predictions based on temp\n",
        "    predictions = predictions / temperature\n",
        "\n",
        "    #we get id of our character\n",
        "    predicted_id = tf.random.categorical(predictions,num_samples=1)[-1,0].numpy()\n",
        "\n",
        "    # we assign next character\n",
        "    input_eval = tf.expand_dims([predicted_id],0)\n",
        "\n",
        "    # we get character and append our output\n",
        "    text_generated.append(ind_to_char[predicted_id])\n",
        "\n",
        "  return (start_seed+\"\".join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iMADG93nQfx1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "f4452462-4b1a-4744-c41b-ba63a51aa9db"
      },
      "source": [
        "print(generate_text(created_model,\"ROMEO\",gen_size=1000))\n",
        "# example output - 1"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ROMEO. Poor shows his bondard, he is now ay arms.\n",
            "  APEMANTUS. Ther. O, what, what?\n",
            "  AGUECHEEK. Is not that a calf?  \n",
            "  CHARMIAN. Good naw, good company. I will be Killia'- I, or being RICHMOND'S house\n",
            "\n",
            "Enter PISANIO, Antonio,\n",
            "     bewitchance\n",
            "    t   gentle lady, in mine against thy talk,\n",
            "    The matter-\n",
            "    Is busily and wash his purse?\n",
            "    One of our mother, and of your end-\n",
            "    But, soft and want, defiechet, alise do hang in't.  \n",
            "  ISABELLA. It is Apalanth too, and fast and loathed witzon\n",
            "     Die, thou shouldst adjoin to woo thee;\n",
            "    Here comes the hot balm of the argument.\n",
            "    Let me see.\n",
            "  ANTIGONUS. If it please the Emperor,'s ears to view,\n",
            "    Can we ne'er to distemper. Fear you so,\n",
            "    Could men express me, and rest here with this,\n",
            "    That e of her\n",
            "    Shall is too short, a prize and Say quickly;\n",
            "    Then farewell. Now sir, I have manklething worse;\n",
            "    For, by thee, where your son small I ends me; straight.\n",
            "ANTIPHOLUS OF MRS. OVERIONE. What says Jupiter\n",
            "    What you have told \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2Kd3MZfQfvO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "01e7a11d-76ba-4646-dfa3-e1be64f7b37d"
      },
      "source": [
        "print(generate_text(created_model,\"JULIET\",gen_size=200))\n",
        "# example output - 2"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JULIET. Sau your daughter?\n",
            "  PAULINA. What, Yea,!\n",
            "    And, for my self-expeciatel was by-\n",
            "    Her bridal-conceite can every wash\n",
            "    All to myself the moon!          Exeunt\n",
            "\n",
            "            Enter a WALES pash o\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2nb07TaQfsp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5tJkgVaQfp1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUJmiVoeQfnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SygaU2KCQfbu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}