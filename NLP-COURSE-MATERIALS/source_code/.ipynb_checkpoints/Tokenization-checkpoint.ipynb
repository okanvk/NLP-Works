{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize,sent_tokenize "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', \"'m\", 'from', 'Turkey', '!']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokenize(\"I'm from Turkey!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Merhaba!', 'Bugün 2. köprü Fsm.de trafik vardı.değil mi?']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(\"Merhaba! Bugün 2. köprü Fsm.de trafik vardı.değil mi?\")\n",
    "#Zemberek\n",
    "#https://github.com/ahmetaa/zemberek-nlp/tree/master/tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "string = \"Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem Ipsum is simply dummy text of the printing and typesetting industry.\n",
      "Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book.\n",
      "It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged.\n",
      "It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.\n"
     ]
    }
   ],
   "source": [
    "for sentence in sent_tokenize(string):\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tk = TweetTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Köpekler',\n",
       " 'bugünlerde',\n",
       " 'çok',\n",
       " 'mutlu.Kediler',\n",
       " 'çok',\n",
       " 'tatlı',\n",
       " 'yaratıklar',\n",
       " '.',\n",
       " '#catsday']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.tokenize(\"Köpekler bugünlerde çok mutlu.Kediler çok tatlı yaratıklar. #catsday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Köpekler',\n",
       " 'bugünlerde',\n",
       " 'çok',\n",
       " 'mutlu',\n",
       " '.',\n",
       " 'Kediler',\n",
       " 'çok',\n",
       " 'tatlı',\n",
       " 'yaratıklar',\n",
       " '.',\n",
       " '#catsday']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tk.tokenize(\"Köpekler bugünlerde çok mutlu. Kediler çok tatlı yaratıklar. #catsday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', \"'\", 'have', 'to', 'go', 'Turkey', '-', 'you', 'can', 'find', 'flight', 'from', 'THY', 'for', '12', '$']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import WordPunctTokenizer \n",
    "punct_tokenizer = WordPunctTokenizer() \n",
    "sentence = \"I'have to go\\n Turkey -\\nyou can find flight from THY for 12$\\n\"\n",
    "result = punct_tokenizer.tokenize(sentence) \n",
    "print(result) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I have to go Turkey you can find flight from for\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import words\n",
    "new_sentence = []\n",
    "corpora = words.words()\n",
    "for token in result:\n",
    "    if token in corpora:\n",
    "        new_sentence.append(token)\n",
    "print(\" \".join(new_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "236736"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(corpora)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
