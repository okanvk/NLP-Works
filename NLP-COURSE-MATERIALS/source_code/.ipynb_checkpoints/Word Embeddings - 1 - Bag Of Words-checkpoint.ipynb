{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2715"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph = \"Thank Thank Thank you all so very much. Thank you to The Academy. I have to congratulate the other incredible nominees in this room for their unbelievable performances.The Revenant was the product of an unbelievable cast and crew I got to work alongside. First off, to my brother in this endeavor, Mr. Tom Hardy. Tom, your fierice talent on screen can only be surpassed off screen by Mr. Alejandro Inarritu. As the history of cinema unfolds, you have forged your way into history this past two years. What an unbelievable talent you are.Thank you to you and [Emmanuel] Chivo [Lubezki] for creating a transcendent cinematic experience for all of us.Thank you to everyone at Fox and Regency. Ana Melching, you were the champion of this endeavor.I have to thank everyone from the very onset of my career â€” Mr. [Unclear name] Jones for casting me in my first film. Mr. Scorsese for teaching me so much about the cinematic art form. To Mr. Rick Yorn, thank you for helping me navigate this industry. And, to my parents, none of this would be possible without you, and, to my friends, I love you dearly, you know who you are.Lorem Ipsum is simply dummy text of the printing and typesetting industry. Lorem Ipsum has been the industry's standard dummy text ever since the 1500s, when an unknown printer took a galley of type and scrambled it to make a type specimen book. It has survived not only five centuries, but also the leap into electronic typesetting, remaining essentially unchanged. It was popularised in the 1960s with the release of Letraset sheets containing Lorem Ipsum passages, and more recently with desktop publishing software like Aldus PageMaker including versions of Lorem Ipsum.Contrary to popular belief, Lorem Ipsum is not simply random text. It has roots in a piece of classical Latin literature from 45 BC, making it over 2000 years old. Richard McClintock, a Latin professor at Hampden-Sydney College in Virginia, looked up one of the more obscure Latin words, consectetur, from a Lorem Ipsum passage, and going through the cites of the word in classical literature, discovered the undoubtable source. Lorem Ipsum comes from sections 1.10.32 and 1.10.33 of de Finibus Bonorum et Malorum (The Extremes of Good and Evil) by Cicero, written in 45 BC. This book is a treatise on the theory of ethics, very popular during the Renaissance. The first line of Lorem Ipsum, comes from a line in section 1.10.32.The standard chunk of Lorem Ipsum used since the 1500s is reproduced below for those interested. Sections 1.10.32 and 1.10.33 from de Finibus Bonorum et Malorum by Cicero are also reproduced in their exact original form, accompanied by English versions from the 1914 translation by H. Rackham.\"\n",
    "lower_paragraph = paragraph.lower()\n",
    "len(lower_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n",
      "thank thank thank you all so very much.\n"
     ]
    }
   ],
   "source": [
    "dataset = nltk.sent_tokenize(lower_paragraph)\n",
    "print(len(dataset))\n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(dataset)):\n",
    "    dataset[i] = re.sub(r\"\\d\",\" \",dataset[i])\n",
    "    dataset[i] = re.sub(r\"\\W\",\" \",dataset[i])\n",
    "    dataset[i] = re.sub(r\"\\s\",\" \",dataset[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thank': 8, 'you': 13, 'all': 2, 'so': 2, 'very': 3, 'much': 2, 'to': 12, 'the': 25, 'academy': 1, 'i': 4, 'have': 3, 'congratulate': 1, 'other': 1, 'incredible': 1, 'nominees': 1, 'in': 10, 'this': 7, 'room': 1, 'for': 7, 'their': 2, 'unbelievable': 3, 'performances': 1, 'revenant': 1, 'was': 2, 'product': 1, 'of': 18, 'an': 3, 'cast': 1, 'and': 12, 'crew': 1, 'got': 1, 'work': 1, 'alongside': 1, 'first': 3, 'off': 2, 'my': 5, 'brother': 1, 'endeavor': 2, 'mr': 5, 'tom': 2, 'hardy': 1, 'your': 2, 'fierice': 1, 'talent': 2, 'on': 2, 'screen': 2, 'can': 1, 'only': 2, 'be': 2, 'surpassed': 1, 'by': 5, 'alejandro': 1, 'inarritu': 1, 'as': 1, 'history': 2, 'cinema': 1, 'unfolds': 1, 'forged': 1, 'way': 1, 'into': 2, 'past': 1, 'two': 1, 'years': 2, 'what': 1, 'are': 3, 'emmanuel': 1, 'chivo': 1, 'lubezki': 1, 'creating': 1, 'a': 8, 'transcendent': 1, 'cinematic': 2, 'experience': 1, 'us': 1, 'everyone': 2, 'at': 2, 'fox': 1, 'regency': 1, 'ana': 1, 'melching': 1, 'were': 1, 'champion': 1, 'from': 7, 'onset': 1, 'career': 1, 'unclear': 1, 'name': 1, 'jones': 1, 'casting': 1, 'me': 3, 'film': 1, 'scorsese': 1, 'teaching': 1, 'about': 1, 'art': 1, 'form': 2, 'rick': 1, 'yorn': 1, 'helping': 1, 'navigate': 1, 'industry': 3, 'parents': 1, 'none': 1, 'would': 1, 'possible': 1, 'without': 1, 'friends': 1, 'love': 1, 'dearly': 1, 'know': 1, 'who': 1, 'lorem': 9, 'ipsum': 9, 'is': 4, 'simply': 2, 'dummy': 2, 'text': 3, 'printing': 1, 'typesetting': 2, 'has': 3, 'been': 1, 's': 4, 'standard': 2, 'ever': 1, 'since': 2, 'when': 1, 'unknown': 1, 'printer': 1, 'took': 1, 'galley': 1, 'type': 2, 'scrambled': 1, 'it': 5, 'make': 1, 'specimen': 1, 'book': 2, 'survived': 1, 'not': 2, 'five': 1, 'centuries': 1, 'but': 1, 'also': 2, 'leap': 1, 'electronic': 1, 'remaining': 1, 'essentially': 1, 'unchanged': 1, 'popularised': 1, 'with': 2, 'release': 1, 'letraset': 1, 'sheets': 1, 'containing': 1, 'passages': 1, 'more': 2, 'recently': 1, 'desktop': 1, 'publishing': 1, 'software': 1, 'like': 1, 'aldus': 1, 'pagemaker': 1, 'including': 1, 'versions': 2, 'contrary': 1, 'popular': 2, 'belief': 1, 'random': 1, 'roots': 1, 'piece': 1, 'classical': 2, 'latin': 3, 'literature': 2, 'bc': 2, 'making': 1, 'over': 1, 'old': 1, 'richard': 1, 'mcclintock': 1, 'professor': 1, 'hampden': 1, 'sydney': 1, 'college': 1, 'virginia': 1, 'looked': 1, 'up': 1, 'one': 1, 'obscure': 1, 'words': 1, 'consectetur': 1, 'passage': 1, 'going': 1, 'through': 1, 'cites': 1, 'word': 1, 'discovered': 1, 'undoubtable': 1, 'source': 1, 'comes': 2, 'sections': 2, 'de': 2, 'finibus': 2, 'bonorum': 2, 'et': 2, 'malorum': 2, 'extremes': 1, 'good': 1, 'evil': 1, 'cicero': 2, 'written': 1, 'treatise': 1, 'theory': 1, 'ethics': 1, 'during': 1, 'renaissance': 1, 'line': 2, 'section': 1, 'chunk': 1, 'used': 1, 'reproduced': 2, 'below': 1, 'those': 1, 'interested': 1, 'exact': 1, 'original': 1, 'accompanied': 1, 'english': 1, 'translation': 1, 'h': 1, 'rackham': 1}\n"
     ]
    }
   ],
   "source": [
    "corpus = {}\n",
    "for sentence in dataset:\n",
    "    words = nltk.word_tokenize(sentence)\n",
    "    for word in words:\n",
    "        if(corpus.get(word) != None):\n",
    "            corpus[word] += 1\n",
    "        else:\n",
    "            corpus[word] = 1\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank 8\n",
      "you 13\n",
      "all 2\n",
      "so 2\n",
      "very 3\n",
      "much 2\n",
      "to 12\n",
      "the 25\n",
      "academy 1\n",
      "i 4\n",
      "have 3\n",
      "congratulate 1\n",
      "other 1\n",
      "incredible 1\n",
      "nominees 1\n",
      "in 10\n",
      "this 7\n",
      "room 1\n",
      "for 7\n",
      "their 2\n",
      "unbelievable 3\n",
      "performances 1\n",
      "revenant 1\n",
      "was 2\n",
      "product 1\n",
      "of 18\n",
      "an 3\n",
      "cast 1\n",
      "and 12\n",
      "crew 1\n",
      "got 1\n",
      "work 1\n",
      "alongside 1\n",
      "first 3\n",
      "off 2\n",
      "my 5\n",
      "brother 1\n",
      "endeavor 2\n",
      "mr 5\n",
      "tom 2\n",
      "hardy 1\n",
      "your 2\n",
      "fierice 1\n",
      "talent 2\n",
      "on 2\n",
      "screen 2\n",
      "can 1\n",
      "only 2\n",
      "be 2\n",
      "surpassed 1\n",
      "by 5\n",
      "alejandro 1\n",
      "inarritu 1\n",
      "as 1\n",
      "history 2\n",
      "cinema 1\n",
      "unfolds 1\n",
      "forged 1\n",
      "way 1\n",
      "into 2\n",
      "past 1\n",
      "two 1\n",
      "years 2\n",
      "what 1\n",
      "are 3\n",
      "emmanuel 1\n",
      "chivo 1\n",
      "lubezki 1\n",
      "creating 1\n",
      "a 8\n",
      "transcendent 1\n",
      "cinematic 2\n",
      "experience 1\n",
      "us 1\n",
      "everyone 2\n",
      "at 2\n",
      "fox 1\n",
      "regency 1\n",
      "ana 1\n",
      "melching 1\n",
      "were 1\n",
      "champion 1\n",
      "from 7\n",
      "onset 1\n",
      "career 1\n",
      "unclear 1\n",
      "name 1\n",
      "jones 1\n",
      "casting 1\n",
      "me 3\n",
      "film 1\n",
      "scorsese 1\n",
      "teaching 1\n",
      "about 1\n",
      "art 1\n",
      "form 2\n",
      "rick 1\n",
      "yorn 1\n",
      "helping 1\n",
      "navigate 1\n",
      "industry 3\n",
      "parents 1\n",
      "none 1\n",
      "would 1\n",
      "possible 1\n",
      "without 1\n",
      "friends 1\n",
      "love 1\n",
      "dearly 1\n",
      "know 1\n",
      "who 1\n",
      "lorem 9\n",
      "ipsum 9\n",
      "is 4\n",
      "simply 2\n",
      "dummy 2\n",
      "text 3\n",
      "printing 1\n",
      "typesetting 2\n",
      "has 3\n",
      "been 1\n",
      "s 4\n",
      "standard 2\n",
      "ever 1\n",
      "since 2\n",
      "when 1\n",
      "unknown 1\n",
      "printer 1\n",
      "took 1\n",
      "galley 1\n",
      "type 2\n",
      "scrambled 1\n",
      "it 5\n",
      "make 1\n",
      "specimen 1\n",
      "book 2\n",
      "survived 1\n",
      "not 2\n",
      "five 1\n",
      "centuries 1\n",
      "but 1\n",
      "also 2\n",
      "leap 1\n",
      "electronic 1\n",
      "remaining 1\n",
      "essentially 1\n",
      "unchanged 1\n",
      "popularised 1\n",
      "with 2\n",
      "release 1\n",
      "letraset 1\n",
      "sheets 1\n",
      "containing 1\n",
      "passages 1\n",
      "more 2\n",
      "recently 1\n",
      "desktop 1\n",
      "publishing 1\n",
      "software 1\n",
      "like 1\n",
      "aldus 1\n",
      "pagemaker 1\n",
      "including 1\n",
      "versions 2\n",
      "contrary 1\n",
      "popular 2\n",
      "belief 1\n",
      "random 1\n",
      "roots 1\n",
      "piece 1\n",
      "classical 2\n",
      "latin 3\n",
      "literature 2\n",
      "bc 2\n",
      "making 1\n",
      "over 1\n",
      "old 1\n",
      "richard 1\n",
      "mcclintock 1\n",
      "professor 1\n",
      "hampden 1\n",
      "sydney 1\n",
      "college 1\n",
      "virginia 1\n",
      "looked 1\n",
      "up 1\n",
      "one 1\n",
      "obscure 1\n",
      "words 1\n",
      "consectetur 1\n",
      "passage 1\n",
      "going 1\n",
      "through 1\n",
      "cites 1\n",
      "word 1\n",
      "discovered 1\n",
      "undoubtable 1\n",
      "source 1\n",
      "comes 2\n",
      "sections 2\n",
      "de 2\n",
      "finibus 2\n",
      "bonorum 2\n",
      "et 2\n",
      "malorum 2\n",
      "extremes 1\n",
      "good 1\n",
      "evil 1\n",
      "cicero 2\n",
      "written 1\n",
      "treatise 1\n",
      "theory 1\n",
      "ethics 1\n",
      "during 1\n",
      "renaissance 1\n",
      "line 2\n",
      "section 1\n",
      "chunk 1\n",
      "used 1\n",
      "reproduced 2\n",
      "below 1\n",
      "those 1\n",
      "interested 1\n",
      "exact 1\n",
      "original 1\n",
      "accompanied 1\n",
      "english 1\n",
      "translation 1\n",
      "h 1\n",
      "rackham 1\n",
      "-----\n",
      "230\n"
     ]
    }
   ],
   "source": [
    "for k,v in corpus.items():\n",
    "    print(k,v)\n",
    "print(\"-----\")\n",
    "print(len(corpus.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['the', 'of', 'you', 'to', 'and', 'in', 'lorem', 'ipsum', 'thank', 'a', 'this', 'for', 'from', 'my', 'mr', 'by', 'it', 'i', 'is', 's', 'very', 'have', 'unbelievable', 'an', 'first', 'are', 'me', 'industry', 'text', 'has', 'latin', 'all', 'so', 'much', 'their', 'was', 'off', 'endeavor', 'tom', 'your', 'talent', 'on', 'screen', 'only', 'be', 'history', 'into', 'years', 'cinematic', 'everyone', 'at', 'form', 'simply', 'dummy', 'typesetting', 'standard', 'since', 'type', 'book', 'not', 'also', 'with', 'more', 'versions', 'popular', 'classical', 'literature', 'bc', 'comes', 'sections', 'de', 'finibus', 'bonorum', 'et', 'malorum', 'cicero', 'line', 'reproduced', 'academy', 'congratulate', 'other', 'incredible', 'nominees', 'room', 'performances', 'revenant', 'product', 'cast', 'crew', 'got', 'work', 'alongside', 'brother', 'hardy', 'fierice', 'can', 'surpassed', 'alejandro', 'inarritu', 'as']\n"
     ]
    }
   ],
   "source": [
    "freq_words = heapq.nlargest(100,corpus,key=corpus.get)\n",
    "print(freq_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_vectors = []\n",
    "count = 0\n",
    "for data in dataset:\n",
    "    vector = []\n",
    "    words = nltk.word_tokenize(data)\n",
    "    for word in freq_words:\n",
    "        if word in words:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    sentence_vectors.append(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "print(sentence_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentence_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vect_to_text(vector):\n",
    "    sentence = []\n",
    "    for i,state in enumerate(vector):\n",
    "        if state == 1:\n",
    "            sentence.append(freq_words[i])\n",
    "    return \" \".join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'you thank very all so much'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect_to_text(sentence_vectors[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'thank': 84, 'you': 98, 'all': 0, 'so': 73, 'very': 94, 'much': 48, 'to': 88, 'the': 85, 'have': 32, 'in': 34, 'this': 87, 'room': 66, 'for': 28, 'their': 86, 'unbelievable': 92, 'revenant': 64, 'was': 95, 'of': 51, 'an': 2, 'cast': 15, 'and': 3, 'first': 27, 'off': 52, 'my': 49, 'brother': 13, 'endeavor': 22, 'mr': 47, 'tom': 89, 'your': 99, 'talent': 81, 'on': 53, 'screen': 68, 'only': 54, 'be': 7, 'surpassed': 78, 'by': 14, 'history': 33, 'into': 36, 'years': 97, 'are': 4, 'cinematic': 17, 'everyone': 25, 'at': 5, 'regency': 59, 'from': 30, 'me': 45, 'teaching': 82, 'form': 29, 'rick': 65, 'industry': 35, 'lorem': 43, 'ipsum': 37, 'is': 38, 'simply': 71, 'dummy': 21, 'text': 83, 'typesetting': 91, 'has': 31, 'been': 8, 'standard': 77, 'since': 72, 'type': 90, 'it': 39, 'specimen': 76, 'book': 12, 'survived': 79, 'not': 50, 'also': 1, 'remaining': 61, 'with': 96, 'release': 60, 'sheets': 70, 'more': 46, 'recently': 58, 'software': 74, 'versions': 93, 'popular': 55, 'belief': 9, 'random': 57, 'roots': 67, 'classical': 18, 'latin': 40, 'literature': 42, 'bc': 6, 'sydney': 80, 'source': 75, 'comes': 19, 'sections': 69, 'de': 20, 'finibus': 26, 'bonorum': 11, 'et': 23, 'malorum': 44, 'cicero': 16, 'ethics': 24, 'renaissance': 62, 'line': 41, 'reproduced': 63, 'below': 10, 'rackham': 56}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=100)\n",
    "vectorizer.fit(dataset)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 100)\n",
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "[[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 1 0 0 0 1 0]]\n"
     ]
    }
   ],
   "source": [
    "vector = vectorizer.transform([dataset[0]])\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "text2 = [\"thank thank kid\"]\n",
    "vector = vectorizer.transform(text2)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
